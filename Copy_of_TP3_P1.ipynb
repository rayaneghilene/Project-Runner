{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rayaneghilene/Project-Runner/blob/main/Copy_of_TP3_P1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 3, Part 1 : N_Gram on WikiText"
      ],
      "metadata": {
        "id": "wk7CUTZFN4uW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqbsta7lIHhz"
      },
      "source": [
        "## Download WikiText-2 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:18.265129Z",
          "start_time": "2020-01-29T18:30:17.460952Z"
        },
        "id": "jfGVtATnIHh2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fe2b952-ff78-4395-b327-43b56c929551"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-02 11:37:16--  https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10797148 (10M) [text/plain]\n",
            "Saving to: ‘train.txt’\n",
            "\n",
            "train.txt           100%[===================>]  10.30M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2023-04-02 11:37:17 (172 MB/s) - ‘train.txt’ saved [10797148/10797148]\n",
            "\n",
            "--2023-04-02 11:37:17--  https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/valid.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1121681 (1.1M) [text/plain]\n",
            "Saving to: ‘valid.txt’\n",
            "\n",
            "valid.txt           100%[===================>]   1.07M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-04-02 11:37:17 (54.6 MB/s) - ‘valid.txt’ saved [1121681/1121681]\n",
            "\n",
            "--2023-04-02 11:37:17--  https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1256449 (1.2M) [text/plain]\n",
            "Saving to: ‘test.txt’\n",
            "\n",
            "test.txt            100%[===================>]   1.20M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-04-02 11:37:18 (61.5 MB/s) - ‘test.txt’ saved [1256449/1256449]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! wget https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/train.txt\n",
        "! wget https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/valid.txt\n",
        "! wget https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/test.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EW7UOlBNIHh5"
      },
      "source": [
        "## Helper function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:18.271133Z",
          "start_time": "2020-01-29T18:30:18.266713Z"
        },
        "id": "u5edQma8IHh7"
      },
      "outputs": [],
      "source": [
        "from collections import Counter, defaultdict\n",
        "import math\n",
        "import copy\n",
        "import random\n",
        "import operator\n",
        "\n",
        "flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "\n",
        "# some helper functions\n",
        "def prepare_data(filename):\n",
        "    data = [l.strip().lower().split() + ['</s>'] for l in open(filename) if l.strip()]\n",
        "    corpus = flatten(data)\n",
        "    vocab = set(corpus)\n",
        "    return vocab, data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# About this lab\n",
        "\n",
        "In this notebook, you will work with the N-Gram Language Model on the Wiki Text Dataset. This first lab should take you approximatively 1h.\n",
        "\n",
        "<div align =\"center\"><img src=\"https://www.w3.org/TR/ngram-spec/tree2.gif\"></div>"
      ],
      "metadata": {
        "id": "iijp5LYrORSL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvjT2fTlIHh9"
      },
      "source": [
        "# A - The N-Gram Language Model\n",
        "\n",
        "A language model assigns a probability to each possible next word given a history of previous words (context). \n",
        "\n",
        "<div align='center'> $P(w_t|w_{t-1}, w_{t-2}, ... , w_1)$ </div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Markov Assumption\n",
        "\n",
        "Since calculating the probability of the whole sentence is not feasible, the Markov Assumption is introduced.\n",
        "\n",
        "It assumes that each next word only depends on the previous K words (in an N-Gram language model, K = N-1).\n",
        "- Unigram: $P(w_t|w_{t-1}, w_{t-2}, ... , w_1) = P(w_t)$\n",
        "- Bigram: $P(w_t|w_{t-1}, w_{t-2}, ... , w_1) = P(w_t|w_{t-1}) $\n",
        "- Trigram: $P(w_t|w_{t-1}, w_{t-2}, ... , w_1) = P(w_t|w_{t-1}, w_{t-2})$\n",
        "\n",
        "For an N-Gram language model, the probability is calculated by counting the frequency:\n",
        "\n",
        "$P(w_t|w_{t-1}, w_{t-2}, ... ,w_{t-N+1}) = \\frac{C(w_t, w_{t-1}, w_{t-2}, ... ,w_{t-N+1} )}{C(w_{t-1}, w_{t-2}, ... ,w_{t-N+1})}$\n",
        "\n",
        "\n",
        "We provide you the code of the NGram Language Model.\n",
        "\n",
        "* Understand the code"
      ],
      "metadata": {
        "id": "77l_VuoMPhzO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:18.290242Z",
          "start_time": "2020-01-29T18:30:18.272506Z"
        },
        "id": "Ql01Z8IiIHh_"
      },
      "outputs": [],
      "source": [
        "\n",
        "class NGramLM():\n",
        "    def __init__(self, N):\n",
        "        self.N = N\n",
        "        self.vocab = set()\n",
        "        self.data = []\n",
        "        self.prob = {}\n",
        "        self.counts = defaultdict(Counter)\n",
        "    \n",
        "    # For N = 1, the probability is stored in a dict   P = prob[next_word]\n",
        "    # For N > 1, the probability is in a nested dict   P = prob[context][next_word]\n",
        "    def train(self, vocab, data, smoothing_k=0):\n",
        "        self.vocab = vocab\n",
        "        self.data = data\n",
        "        self.smoothing_k = smoothing_k\n",
        "\n",
        "        if self.N == 1:\n",
        "            self.counts = Counter(flatten(data))\n",
        "            self.prob = self.get_prob(self.counts)\n",
        "        else:\n",
        "            self.vocab.add('<s>')\n",
        "            counts = self.count_ngram()\n",
        "            \n",
        "            self.prob = {}\n",
        "            for context, counter in counts.items():\n",
        "                self.prob[context] = self.get_prob(counter)\n",
        "    \n",
        "    def count_ngram(self):\n",
        "        counts = defaultdict(Counter)\n",
        "        for sentence in self.data:\n",
        "            sentence = (self.N - 1) * ['<s>'] + sentence \n",
        "            for i in range(len(sentence)-self.N+1):\n",
        "                context = sentence[i:i+self.N-1]\n",
        "                context = \" \".join(context)\n",
        "    \n",
        "    # For N = 1, the probability is stored in a dict   P = prob[next_word\n",
        "                word = sentence[i+self.N-1]\n",
        "                counts[context][word] += 1\n",
        "\n",
        "        self.counts = counts\n",
        "        return counts\n",
        "        \n",
        "    # normalize counts into probability(considering smoothing)\n",
        "    def get_prob(self, counter):\n",
        "        total = float(sum(counter.values()))\n",
        "        k = self.smoothing_k\n",
        "    \n",
        "    # For N = 1, the probability is stored in a dict   P = prob[next_word\n",
        "        \n",
        "        prob = {}\n",
        "        for word, count in counter.items():\n",
        "            prob[word] = (count + k) / (total + len(self.vocab) * k)\n",
        "        return prob\n",
        "        \n",
        "    def get_ngram_logprob(self, word, seq_len, context=\"\"):\n",
        "        if self.N == 1 and word in self.prob.keys():\n",
        "            return math.log(self.prob[word]) / seq_len\n",
        "        elif self.N > 1 and not self._is_unseen_ngram(context, word):\n",
        "            return math.log(self.prob[context][word]) / seq_len\n",
        "        else:\n",
        "            # assign a small probability to the unseen ngram\n",
        "            # to avoid log of zero and to penalise unseen word or context\n",
        "            return math.log(1/len(self.vocab)) / seq_len\n",
        "        \n",
        "    def get_ngram_prob(self, word, context=\"\"):\n",
        "        if self.N == 1 and word in self.prob.keys():\n",
        "            return self.prob[word]\n",
        "        elif self.N > 1 and not self._is_unseen_ngram(context, word):\n",
        "            return self.prob[context][word]\n",
        "        elif word in self.vocab and self.smoothing_k > 0:\n",
        "            # probability assigned by smoothing\n",
        "            return self.smoothing_k / (sum(self.counts[context].values()) + self.smoothing_k*len(self.vocab))\n",
        "        else:\n",
        "            # unseen word or context\n",
        "            return 0\n",
        "            \n",
        "    # In this method, the perplexity is measured at the sentence-level, averaging over all sentences.\n",
        "    # Actually, it is also possible to calculate perplexity by merging all sentences into a long one.\n",
        "    def perplexity(self, test_data):\n",
        "        log_ppl = 0\n",
        "        if self.N == 1:\n",
        "            for sentence in test_data:\n",
        "                for word in sentence:\n",
        "                    log_ppl += self.get_ngram_logprob(word=word, seq_len=len(sentence))\n",
        "        else:\n",
        "            for sentence in test_data:\n",
        "                for i in range(len(sentence)-self.N+1):\n",
        "                    context = sentence[i:i+self.N-1]\n",
        "                    context = \" \".join(context)\n",
        "                    word = sentence[i+self.N-1]\n",
        "                    log_ppl += self.get_ngram_logprob(context=context, word=word, seq_len=len(sentence))\n",
        "        log_ppl /= len(test_data)\n",
        "        ppl = math.exp(-log_ppl)\n",
        "        return ppl\n",
        "    \n",
        "    def _is_unseen_ngram(self, context, word):\n",
        "        if context not in self.prob.keys() or word not in self.prob[context].keys():\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    \n",
        "    # generate the most probable k words\n",
        "    def generate_next(self, context, k):\n",
        "        context = (self.N-1) * '<s> ' + context\n",
        "        context = context.split()\n",
        "        ngram_context_list = context[-self.N+1:]\n",
        "        ngram_context = \" \".join(ngram_context_list)\n",
        "        \n",
        "        if ngram_context in self.prob.keys():\n",
        "            candidates = self.prob[ngram_context]\n",
        "            most_probable_words = sorted(candidates.items(), key=lambda kv: kv[1], reverse=True)\n",
        "            for i in range(min(k, len(most_probable_words))):\n",
        "                print(\" \".join(context[self.N-1:])+\" \"+most_probable_words[i][0]+\"\\t P={}\".format(most_probable_words[i][1]))\n",
        "        else:\n",
        "            print(\"Unseen context!\")\n",
        "            \n",
        "    # generate the next n words with greedy search\n",
        "    def generate_next_n(self, context, n):\n",
        "        context = (self.N-1) * '<s> ' + context\n",
        "        context = context.split()\n",
        "        ngram_context_list = context[-self.N+1:]\n",
        "        ngram_context = \" \".join(ngram_context_list)\n",
        "        \n",
        "        for i in range(n):\n",
        "            try:\n",
        "                candidates = self.prob[ngram_context]\n",
        "                most_likely_next = max(candidates.items(), key=operator.itemgetter(1))[0]\n",
        "                context += [most_likely_next]\n",
        "                ngram_context_list = ngram_context_list[1:] + [most_likely_next]\n",
        "                ngram_context = \" \".join(ngram_context_list)\n",
        "            except:\n",
        "                break\n",
        "        print(\" \".join(context[self.N-1:]))\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VwN9CXyIHiC"
      },
      "source": [
        "# B -Train with toy dataset\n",
        "\n",
        "As the model is created we can train a N-Gram LM. At this step, let's train a Bigram language model on the toy dataset.\n",
        "\n",
        "* What preprocessing was done ?\n",
        "* How many word do we have in our vocabulary ?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The preprocessing here is to have our data as a form a \n",
        "matrix each word corresponds to an element of this matrix and each line is a sentence . \n",
        "\n",
        "* We have 7 words in our vocabulary "
      ],
      "metadata": {
        "id": "WErKuz0iImkt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:18.297355Z",
          "start_time": "2020-01-29T18:30:18.291387Z"
        },
        "id": "sabDGGntIHiC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07aaecfe-b86e-4e5a-fff0-350445a7698e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['i', 'like', 'whipped', 'cream', '</s>'], ['i', 'like', 'candys', '</s>'], ['i', 'hate', 'tomatoes', '</s>']]\n",
            "{'hate', 'tomatoes', 'candys', 'whipped', 'cream', '</s>', 'i', 'like'}\n"
          ]
        }
      ],
      "source": [
        "corpus = [\"I like whipped cream\",\n",
        "         \"I like candys\",\n",
        "         \"I hate tomatoes\"]\n",
        "data = [l.strip().lower().split() + ['</s>'] for l in corpus if l.strip()]\n",
        "vocab = set(flatten(data))\n",
        "\n",
        "# TODO : Answer the questions\n",
        "print(data)\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:18.301895Z",
          "start_time": "2020-01-29T18:30:18.298317Z"
        },
        "id": "-HKqP6zvIHiE"
      },
      "outputs": [],
      "source": [
        "def print_probability(lm):\n",
        "    for context in lm.vocab:\n",
        "        for word in lm.vocab:\n",
        "            prob = lm.get_ngram_prob(word, context)\n",
        "            print(\"P({}\\t|{}) = {}\".format(word, context, prob))\n",
        "        print(\"--------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y-jHF6bIHiF"
      },
      "source": [
        "# C - Smoothing\n",
        "Smoothing method is used to deal with the sparsity problem in N-Gram language modelling.\n",
        "The probability mass is shifted towards the less frequent words.\n",
        "\n",
        "For example, with an add-1 smoothing, the probability is calculated as:\n",
        "\n",
        "$$P(w_t | context) = \\frac{C(w_t, context)+1}{C(context)+|V|}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1: What is the disadvantage of smoothing?**\n",
        "\n",
        "A:Smoothing can lead to overfitting, where the language model becomes too specific to the training data and does not generalize well to new data. This can be particularly problematic if the training data is limited, as the language model may not capture the true distribution of N-grams in the language."
      ],
      "metadata": {
        "id": "fQB4IJYPQei1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:18.310269Z",
          "start_time": "2020-01-29T18:30:18.302761Z"
        },
        "id": "nBS-MIWiIHiG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d340a69-e765-41a3-dae2-b89c3e394482"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P(hate\t|hate) = 0.10526315789473684\n",
            "P(tomatoes\t|hate) = 0.15789473684210525\n",
            "P(candys\t|hate) = 0.10526315789473684\n",
            "P(whipped\t|hate) = 0.10526315789473684\n",
            "P(cream\t|hate) = 0.10526315789473684\n",
            "P(</s>\t|hate) = 0.10526315789473684\n",
            "P(i\t|hate) = 0.10526315789473684\n",
            "P(<s>\t|hate) = 0.10526315789473684\n",
            "P(like\t|hate) = 0.10526315789473684\n",
            "--------------------------\n",
            "P(hate\t|tomatoes) = 0.10526315789473684\n",
            "P(tomatoes\t|tomatoes) = 0.10526315789473684\n",
            "P(candys\t|tomatoes) = 0.10526315789473684\n",
            "P(whipped\t|tomatoes) = 0.10526315789473684\n",
            "P(cream\t|tomatoes) = 0.10526315789473684\n",
            "P(</s>\t|tomatoes) = 0.15789473684210525\n",
            "P(i\t|tomatoes) = 0.10526315789473684\n",
            "P(<s>\t|tomatoes) = 0.10526315789473684\n",
            "P(like\t|tomatoes) = 0.10526315789473684\n",
            "--------------------------\n",
            "P(hate\t|candys) = 0.10526315789473684\n",
            "P(tomatoes\t|candys) = 0.10526315789473684\n",
            "P(candys\t|candys) = 0.10526315789473684\n",
            "P(whipped\t|candys) = 0.10526315789473684\n",
            "P(cream\t|candys) = 0.10526315789473684\n",
            "P(</s>\t|candys) = 0.15789473684210525\n",
            "P(i\t|candys) = 0.10526315789473684\n",
            "P(<s>\t|candys) = 0.10526315789473684\n",
            "P(like\t|candys) = 0.10526315789473684\n",
            "--------------------------\n",
            "P(hate\t|whipped) = 0.10526315789473684\n",
            "P(tomatoes\t|whipped) = 0.10526315789473684\n",
            "P(candys\t|whipped) = 0.10526315789473684\n",
            "P(whipped\t|whipped) = 0.10526315789473684\n",
            "P(cream\t|whipped) = 0.15789473684210525\n",
            "P(</s>\t|whipped) = 0.10526315789473684\n",
            "P(i\t|whipped) = 0.10526315789473684\n",
            "P(<s>\t|whipped) = 0.10526315789473684\n",
            "P(like\t|whipped) = 0.10526315789473684\n",
            "--------------------------\n",
            "P(hate\t|cream) = 0.10526315789473684\n",
            "P(tomatoes\t|cream) = 0.10526315789473684\n",
            "P(candys\t|cream) = 0.10526315789473684\n",
            "P(whipped\t|cream) = 0.10526315789473684\n",
            "P(cream\t|cream) = 0.10526315789473684\n",
            "P(</s>\t|cream) = 0.15789473684210525\n",
            "P(i\t|cream) = 0.10526315789473684\n",
            "P(<s>\t|cream) = 0.10526315789473684\n",
            "P(like\t|cream) = 0.10526315789473684\n",
            "--------------------------\n",
            "P(hate\t|</s>) = 0.1111111111111111\n",
            "P(tomatoes\t|</s>) = 0.1111111111111111\n",
            "P(candys\t|</s>) = 0.1111111111111111\n",
            "P(whipped\t|</s>) = 0.1111111111111111\n",
            "P(cream\t|</s>) = 0.1111111111111111\n",
            "P(</s>\t|</s>) = 0.1111111111111111\n",
            "P(i\t|</s>) = 0.1111111111111111\n",
            "P(<s>\t|</s>) = 0.1111111111111111\n",
            "P(like\t|</s>) = 0.1111111111111111\n",
            "--------------------------\n",
            "P(hate\t|i) = 0.14285714285714285\n",
            "P(tomatoes\t|i) = 0.09523809523809523\n",
            "P(candys\t|i) = 0.09523809523809523\n",
            "P(whipped\t|i) = 0.09523809523809523\n",
            "P(cream\t|i) = 0.09523809523809523\n",
            "P(</s>\t|i) = 0.09523809523809523\n",
            "P(i\t|i) = 0.09523809523809523\n",
            "P(<s>\t|i) = 0.09523809523809523\n",
            "P(like\t|i) = 0.19047619047619047\n",
            "--------------------------\n",
            "P(hate\t|<s>) = 0.09523809523809523\n",
            "P(tomatoes\t|<s>) = 0.09523809523809523\n",
            "P(candys\t|<s>) = 0.09523809523809523\n",
            "P(whipped\t|<s>) = 0.09523809523809523\n",
            "P(cream\t|<s>) = 0.09523809523809523\n",
            "P(</s>\t|<s>) = 0.09523809523809523\n",
            "P(i\t|<s>) = 0.23809523809523808\n",
            "P(<s>\t|<s>) = 0.09523809523809523\n",
            "P(like\t|<s>) = 0.09523809523809523\n",
            "--------------------------\n",
            "P(hate\t|like) = 0.1\n",
            "P(tomatoes\t|like) = 0.1\n",
            "P(candys\t|like) = 0.15\n",
            "P(whipped\t|like) = 0.15\n",
            "P(cream\t|like) = 0.1\n",
            "P(</s>\t|like) = 0.1\n",
            "P(i\t|like) = 0.1\n",
            "P(<s>\t|like) = 0.1\n",
            "P(like\t|like) = 0.1\n",
            "--------------------------\n"
          ]
        }
      ],
      "source": [
        "lm = NGramLM(2)\n",
        "lm.train(vocab, data, smoothing_k=0)\n",
        "\n",
        "#print_probability(lm)\n",
        "########################################################\n",
        "# TODO: try with add-2 smoothing and see the probability\n",
        "########################################################\n",
        "lm.train(vocab, data, smoothing_k=2)\n",
        "\n",
        "print_probability(lm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG35kkhVIHiH"
      },
      "source": [
        "# D -Train on WikiText-2 dataset and Evaluate Perplexity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating perplexity\n",
        "\n",
        "We define the perplexity as \n",
        "\n",
        "<div align=\"center\">\n",
        "$ PPL(W) = P(w_1, w_2, ... , w_n)^{-\\frac{1}{n}}$\n",
        "\n",
        "$ log(PPL(W)) = -\\frac{1}{n}\\sum^n_{k=1}log(P(w_k|w_1, w_2, ... , w_{k-1}))$\n",
        "</div>\n",
        "\n"
      ],
      "metadata": {
        "id": "q2DYmW1UQ5hF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Q2: Why do we need to calculate log perplexity?**\n",
        "\n",
        "A: The perplexity score is calculated by taking the exponential of the cross-entropy loss, which can lead to numerical instability when working with small probabilities. Taking the logarithm of the perplexity score converts the multiplication of probabilities into addition, which is more numerically stable."
      ],
      "metadata": {
        "id": "4G5C7IV_RLzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab, train_data = prepare_data('train.txt')\n",
        "valid_vocab, valid_data = prepare_data('valid.txt')\n",
        "test_vocab, test_data = prepare_data('test.txt')\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "id": "4oYHXrWqRYLP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfd8f74d-efa2-419e-dce9-f82ee46fe2d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28912\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:22.224402Z",
          "start_time": "2020-01-29T18:30:18.714081Z"
        },
        "id": "rYcGOj6cIHiI"
      },
      "outputs": [],
      "source": [
        "lm = NGramLM(3)\n",
        "\n",
        "# Train the model using vocab and training data\n",
        "\n",
        "lm.train(vocab,train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:22.811004Z",
          "start_time": "2020-01-29T18:30:22.225566Z"
        },
        "id": "Qh0NmO16IHiI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc29f06b-94bc-4b63-f7e0-866119299fa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "387.1190374834364\n",
            "310.6880368161233\n"
          ]
        }
      ],
      "source": [
        "print(lm.perplexity(valid_data))\n",
        "print(lm.perplexity(test_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zw9EMl6XIHiK"
      },
      "source": [
        "# E - Generating sentences\n",
        "\n",
        "With a pre-trained N-Gram language model, we can predict possible next words given context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:22.815241Z",
          "start_time": "2020-01-29T18:30:22.812141Z"
        },
        "id": "LVE1LyfxIHiL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed0115fe-b2c2-4965-d967-3bf7b6b95222"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the eggs hatch at night , and the larvae swim to the water surface where they drift with the ocean currents , preying on <unk> . this stage involves three <unk> and lasts for 15 – 35 days . after the third moult , the juvenile takes on a form closer to the adult , and adopts a <unk> lifestyle . the juveniles are rarely seen in the wild , and are poorly known , although they are known to be capable of digging extensive burrows . it is estimated that only 1 larva in every 20 @,@ 000 survives to the <unk> phase . when they reach a carapace length of 15 mm ( 0 @.@ 59 in ) , the juveniles leave their burrows and start their adult lives . </s>\n",
            "the eggs hatch at night , and the larvae swim to the water surface where they were\t P=0.12408759124087591\n",
            "the eggs hatch at night , and the larvae swim to the water surface where they are\t P=0.08029197080291971\n",
            "the eggs hatch at night , and the larvae swim to the water surface where they would\t P=0.051094890510948905\n"
          ]
        }
      ],
      "source": [
        "# generate the most probable following words given the context\n",
        "print(\" \".join(valid_data[12]))\n",
        "\n",
        "# actually the only useful context in the Trigram language model is [\"where\", \"they\"]\n",
        "context = \"the eggs hatch at night , and the larvae swim to the water surface where they\"  \n",
        "lm.generate_next(context, 3)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can also generate with shorter contexts, even shorter than N-1\n",
        "\n",
        "contexts = [\"the eggs\",\n",
        "            \"the\",\n",
        "            \"\"]\n",
        "for context in contexts:\n",
        "  lm.generate_next(context, 3)\n",
        "  print(\"---\")"
      ],
      "metadata": {
        "id": "e1y3MW9DZ-nb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65b1ff4f-044d-4c98-ed38-a27acd58c592"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the eggs hatch\t P=0.18181818181818182\n",
            "the eggs of\t P=0.18181818181818182\n",
            "the eggs are\t P=0.13636363636363635\n",
            "---\n",
            "the first\t P=0.03398926654740608\n",
            "the <unk>\t P=0.020274299344066785\n",
            "the episode\t P=0.015802027429934407\n",
            "---\n",
            " =\t P=0.26132873311734756\n",
            " the\t P=0.14112004039214035\n",
            " in\t P=0.06353347077881095\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:22.828378Z",
          "start_time": "2020-01-29T18:30:22.816839Z"
        },
        "id": "sIiRSWPoIHiM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65a01490-01d4-41d9-e434-908461db7711"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the eggs hatch at night , and the larvae swim to the water surface where they were not able to get the part of the <unk>\n",
            "the eggs hatch at night , and the larvae swim to the water surface where they were not able to get the part of the <unk> of the <unk> of the <unk> of the <unk> of\n"
          ]
        }
      ],
      "source": [
        "context = \"the eggs hatch at night , and the larvae swim to the water surface where they\"  \n",
        "\n",
        "# generate the next n words with greedy search\n",
        "lm.generate_next_n(context, 10)\n",
        "\n",
        "# This is not a good method in practice,\n",
        "# because wrong predictions in the early steps would introduce errors to the following predictions\n",
        "lm.generate_next_n(context, 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSEVmuouIHiN"
      },
      "source": [
        "# F - Effect of N\n",
        "\n",
        "**Q3: Why does the perplexity increase when N is large?**\n",
        "\n",
        "A: because this results in sparsity(many of these N-grams are not present in the training data, resulting in sparse estimates of their probabilities) and overfitting issues that lead to higher perplexity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:41.930826Z",
          "start_time": "2020-01-29T18:30:22.829892Z"
        },
        "id": "wLVeKE_JIHiN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7278b10-e458-4b6d-e02b-95d7b765aa3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "************************\n",
            "1-gram LM perplexity on valid set: 599.6450225274759\n",
            "1-gram LM perplexity on test  set: 553.0467265986567\n",
            "************************\n",
            "2-gram LM perplexity on valid set: 130.75949726991627\n",
            "2-gram LM perplexity on test  set: 114.76592406312065\n",
            "************************\n",
            "3-gram LM perplexity on valid set: 387.1190374834364\n",
            "3-gram LM perplexity on test  set: 310.6880368161233\n",
            "************************\n",
            "4-gram LM perplexity on valid set: 1091.9348578736633\n",
            "4-gram LM perplexity on test  set: 846.7563102382584\n",
            "************************\n",
            "5-gram LM perplexity on valid set: 1558.823545905558\n",
            "5-gram LM perplexity on test  set: 1201.746345143052\n"
          ]
        }
      ],
      "source": [
        "for n in range(1,6):\n",
        "    lm = NGramLM(n)\n",
        "    lm.train(vocab, train_data)\n",
        "    print(\"************************\")\n",
        "    print(\"{}-gram LM perplexity on valid set: {}\".format(n, lm.perplexity(valid_data)))\n",
        "    print(\"{}-gram LM perplexity on test  set: {}\".format(n, lm.perplexity(test_data)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgM5mePhIHiO"
      },
      "source": [
        "# G - Interpolation\n",
        "In interpolation, we mix the probability estimates from all the n-gram estimators to alleviate the sparsity problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:41.937643Z",
          "start_time": "2020-01-29T18:30:41.932047Z"
        },
        "id": "5f57uTZvIHiO"
      },
      "outputs": [],
      "source": [
        "class InterpolateNGramLM(NGramLM):\n",
        "    \n",
        "    def __init__(self, N):\n",
        "        super(InterpolateNGramLM, self).__init__(N)\n",
        "        self.ngram_lms = []\n",
        "        self.lambdas = []\n",
        "        \n",
        "    def train(self, vocab, data, smoothing_k=0, lambdas=[]):\n",
        "        assert len(lambdas) == self.N\n",
        "        assert sum(lambdas) - 1 < 1e-9\n",
        "        self.vocab = vocab\n",
        "        self.lambdas = lambdas\n",
        "        \n",
        "        for i in range(self.N, 0, -1):\n",
        "            # Instanciate the model\n",
        "            lm = NGramLM(i)\n",
        "            print(f\"Training {i}-gram language model\".format(i))\n",
        "            lm.train(vocab, data, smoothing_k)\n",
        "            self.ngram_lms.append(lm)\n",
        "    \n",
        "    def get_ngram_logprob(self, word, seq_len, context):\n",
        "        prob = 0\n",
        "        for i, (coef, lm) in enumerate(zip(self.lambdas, self.ngram_lms)):\n",
        "            context_words = context.split()\n",
        "            cutted_context = \" \".join(context_words[-self.N + i + 1:])\n",
        "            prob += coef * lm.get_ngram_prob(context=cutted_context, word=word)\n",
        "        return math.log(prob) / seq_len\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:46.507705Z",
          "start_time": "2020-01-29T18:30:41.939076Z"
        },
        "id": "6TafZ7KpIHiO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c64a1eab-2889-4359-96e7-123f651698c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training 3-gram language model\n",
            "Training 2-gram language model\n",
            "Training 1-gram language model\n"
          ]
        }
      ],
      "source": [
        "###################################################\n",
        "# Q5: tune your coefficients to decrease perplexity\n",
        "###################################################\n",
        "ilm = InterpolateNGramLM(3)\n",
        "ilm.train(vocab, train_data, lambdas=[0.2, 0.7, 0.1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lambda_grid = [\n",
        "   [0.5, 0.4, 0.1],\n",
        "    [0.4, 0.5, 0.1],\n",
        "    [0.3, 0.6, 0.1],\n",
        "    [0.2, 0.7, 0.1],\n",
        "    [0.1, 0.8, 0.1],\n",
        "]\n",
        "# Train and evaluate the model for each set of lambda values\n",
        "best_lambda = None\n",
        "best_perplexity = float('inf')\n",
        "for lambdas in lambda_grid:\n",
        "    ilm = InterpolateNGramLM(3)\n",
        "    ilm.train(vocab, train_data, lambdas=lambdas)\n",
        "    perplexity = ilm.perplexity(valid_data)\n",
        "    \n",
        "    print(f\"Lambdas: {lambdas}, Perplexity: {perplexity}\")\n",
        "    \n",
        "    if perplexity < best_perplexity:\n",
        "        best_lambda = lambdas\n",
        "        best_perplexity = perplexity\n",
        "        \n",
        "# Train a final model with the best lambda values\n",
        "final_model = InterpolateNGramLM(3)\n",
        "final_model.train(vocab, train_data, lambdas=best_lambda)\n",
        "test_perplexity = final_model.perplexity(test_data)\n",
        "print(f\"Test perplexity: {test_perplexity}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aq3K_wS-TYIa",
        "outputId": "0549bac6-05c1-4fcf-9e31-d1ec49b55d16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training 3-gram language model\n",
            "Training 2-gram language model\n",
            "Training 1-gram language model\n",
            "Lambdas: [0.5, 0.4, 0.1], Perplexity: 126.26886448793077\n",
            "Training 3-gram language model\n",
            "Training 2-gram language model\n",
            "Training 1-gram language model\n",
            "Lambdas: [0.4, 0.5, 0.1], Perplexity: 121.91287165028098\n",
            "Training 3-gram language model\n",
            "Training 2-gram language model\n",
            "Training 1-gram language model\n",
            "Lambdas: [0.3, 0.6, 0.1], Perplexity: 119.58096039943413\n",
            "Training 3-gram language model\n",
            "Training 2-gram language model\n",
            "Training 1-gram language model\n",
            "Lambdas: [0.2, 0.7, 0.1], Perplexity: 119.11642110884343\n",
            "Training 3-gram language model\n",
            "Training 2-gram language model\n",
            "Training 1-gram language model\n",
            "Lambdas: [0.1, 0.8, 0.1], Perplexity: 121.09118796605253\n",
            "Training 3-gram language model\n",
            "Training 2-gram language model\n",
            "Training 1-gram language model\n",
            "Test perplexity: 97.66795707054055\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-29T18:30:48.376911Z",
          "start_time": "2020-01-29T18:30:46.509150Z"
        },
        "id": "UZGf_gQOIHiP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99391116-aeee-48d9-c64b-17291555f979"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "121.09118796605253\n",
            "99.44721686710868\n"
          ]
        }
      ],
      "source": [
        "print(ilm.perplexity(valid_data))\n",
        "print(ilm.perplexity(test_data))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ql7twpklwdtO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "file_extension": ".py",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "version": 3,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}